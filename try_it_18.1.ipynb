{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models and Vectorization Strategies for Text Classification\n",
    "\n",
    "This try-it focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in scikitlearn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n",
    "\n",
    "**Note:** The original dataset contains 200K rows of data. It is best to try to use the full dtaset. If the original dataset is too large for your computer, please use the 'dataset-minimal.csv', which has been reduced to 100K."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:16:22.034081Z",
     "start_time": "2024-08-27T00:16:22.031721Z"
    }
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import vectorizers"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:16:22.188701Z",
     "start_time": "2024-08-27T00:16:22.035420Z"
    }
   },
   "source": [
    "df = pd.read_csv('text_data/dataset.csv')"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:16:22.193296Z",
     "start_time": "2024-08-27T00:16:22.189764Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:19:49.890282Z",
     "start_time": "2024-08-27T00:19:49.513353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/george.li/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/george.li/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/george.li/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:19:50.793286Z",
     "start_time": "2024-08-27T00:19:50.788195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(text, method='stem'):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    if method == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    elif method == 'lemma':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join([lemmatizer.lemmatize(token) for token in tokens])"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:20:26.621455Z",
     "start_time": "2024-08-27T00:19:51.550916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply preprocessing\n",
    "df['stemmed'] = df['text'].apply(lambda x: preprocess(x, 'stem'))\n",
    "df['lemmatized'] = df['text'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "\n",
    "# Split the data\n",
    "X_stem = df['stemmed']\n",
    "X_lemma = df['lemmatized']\n",
    "y = df['humor']\n",
    "X_stem_train, X_stem_test, X_lemma_train, X_lemma_test, y_train, y_test = train_test_split(X_stem, X_lemma, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T00:39:11.316640Z",
     "start_time": "2024-08-27T00:31:21.100061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define pipelines and parameter grids for each model\n",
    "pipelines = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'DecisionTreeClassifier': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'MultinomialNB': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'vectorizer__max_features': [1000, 2000],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'vectorizer__max_features': [1000, 2000],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "        'classifier__max_depth': [5, 10, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        'vectorizer__max_features': [1000, 2000],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "        'classifier__alpha': [0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform grid search\n",
    "def grid_search(X_train, y_train, pipeline, param_grid):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Perform grid search for each preprocessing method and model\n",
    "results = []\n",
    "for prep_method, X_train in [('Stemming', X_stem_train), ('Lemmatization', X_lemma_train)]:\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"\\nPerforming grid search for {prep_method} + {model_name}\")\n",
    "        start_time = time.time()\n",
    "        best_estimator, best_params, best_score = grid_search(X_train, y_train, pipeline, param_grids[model_name])\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'Preprocessing': prep_method,\n",
    "            'Model': model_name,\n",
    "            'Best Score': best_score,\n",
    "            'Best Parameters': best_params,\n",
    "            'Duration': duration\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and sort\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Best Score', ascending=False)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Get the best overall model\n",
    "best_result = results_df.iloc[0]\n",
    "best_prep = best_result['Preprocessing']\n",
    "best_model = best_result['Model']\n",
    "best_params = best_result['Best Parameters']\n",
    "\n",
    "print(f\"\\nBest overall model: {best_prep} + {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the best model on the full training set and evaluate on test set\n",
    "X_train = X_stem_train if best_prep == 'Stemming' else X_lemma_train\n",
    "X_test = X_stem_test if best_prep == 'Stemming' else X_lemma_test\n",
    "\n",
    "best_pipeline = pipelines[best_model]\n",
    "best_pipeline.set_params(**best_params)\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest set accuracy of best model: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print top features for the best model\n",
    "if best_model in ['LogisticRegression', 'DecisionTreeClassifier']:\n",
    "    vectorizer = best_pipeline.named_steps['vectorizer']\n",
    "    classifier = best_pipeline.named_steps['classifier']\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    if best_model == 'LogisticRegression':\n",
    "        importances = classifier.coef_[0]\n",
    "    else:  # DecisionTreeClassifier\n",
    "        importances = classifier.feature_importances_\n",
    "    \n",
    "    top_features = sorted(zip(feature_names, importances), key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 features for {best_prep} + {best_model}:\")\n",
    "    for feature, importance in top_features:\n",
    "        print(f\"{feature}: {importance}\")\n",
    "else:\n",
    "    print(f\"\\nFeature importance not available for {best_model}\")\n",
    "\n",
    "# Print scikit-learn version\n",
    "import sklearn\n",
    "print(f\"\\nscikit-learn version: {sklearn.__version__}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for Stemming + LogisticRegression\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "60 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.834525   0.834625\n",
      " 0.85545    0.85600625        nan        nan        nan        nan\n",
      " 0.8361125  0.8360625  0.860425   0.861875          nan        nan\n",
      "        nan        nan 0.8361125  0.835825   0.86084375 0.86205625]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for Stemming + DecisionTreeClassifier\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Performing grid search for Stemming + MultinomialNB\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for Lemmatization + LogisticRegression\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "60 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.83390625 0.8332125\n",
      " 0.85573125 0.85671875        nan        nan        nan        nan\n",
      " 0.83519375 0.83498125 0.86176875 0.86206875        nan        nan\n",
      "        nan        nan 0.83493125 0.8350875  0.86165625 0.86165625]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for Lemmatization + DecisionTreeClassifier\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/george.li/Documents/code/homework/uci-classifiers/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for Lemmatization + MultinomialNB\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Grid Search Results:\n",
      "Preprocessing                  Model  Best Score                                                                                                                          Best Parameters   Duration\n",
      "Lemmatization     LogisticRegression    0.862069                   {'classifier__C': 1, 'classifier__penalty': 'l2', 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2)}  30.084195\n",
      "     Stemming     LogisticRegression    0.862056                  {'classifier__C': 10, 'classifier__penalty': 'l2', 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2)}  30.391342\n",
      "Lemmatization          MultinomialNB    0.853237                                          {'classifier__alpha': 1.0, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 1)}  14.865007\n",
      "     Stemming          MultinomialNB    0.852831                                          {'classifier__alpha': 1.0, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 1)}  14.999205\n",
      "Lemmatization DecisionTreeClassifier    0.798012 {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2)} 180.882213\n",
      "     Stemming DecisionTreeClassifier    0.794869 {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 1)} 196.044783\n",
      "\n",
      "Best overall model: Lemmatization + LogisticRegression\n",
      "Best parameters: {'classifier__C': 1, 'classifier__penalty': 'l2', 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2)}\n",
      "\n",
      "Test set accuracy of best model: 0.8649\n",
      "\n",
      "Top 10 features for Lemmatization + LogisticRegression:\n",
      "photo: -10.353433633044704\n",
      "video: -7.516810245188317\n",
      "call: 7.371857144338846\n",
      "favourite: 6.617212247281195\n",
      "shit: 6.368356730570359\n",
      "trump: -6.326649424561303\n",
      "hear: 6.081770303369192\n",
      "difference: 5.9646659574682745\n",
      "joke: 5.887148054140139\n",
      "recipe: -5.75392067923298\n",
      "\n",
      "scikit-learn version: 1.5.1\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame({'model': ['Logistic', 'Decision Tree', 'Bayes'], \n",
    "             'best_params': ['', '', ''],\n",
    "             'best_score': ['', '', '']}).set_index('model')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
